---
title: 1: Basic Notions and Notations
published: 2016-12-26 00:00:00
---

Preface
=======

Welcome to part 2 of ["The Burge School of Functional
Programming"](http://jmct.cc/burge.html). This post is where we start to dive
into the substance of the book by introducing the language Burge uses throughout
the book.[^1] It is worth taking a moment to emphasize
something: The goal here is not to parrot or rephrase the contents of the book,
but instead to highlight the lessons that help us think like functional
programmers.

[^1]: As far as I know, no implementation of this language exists, though it
wouldn't be _too_ hard to write one.

Throughout the series I will be using the notation that Burge used. That
being said, I aim to point out when those notations differ from modern
conventions. If I've missed a spot, or a notation is confusing, please let me
know and I'll clarify as quickly as possible.

I'm going to quote directly from the book a lot early on in this post, as I feel
there's a lot of insight to be found at the beginning of Burge's first chapter.
In future posts we'll be able to leverage the vocabulary we get from Burge early
on and we won't need to quote him directly so much.

Similarly, this is likely to be the longest post in the series, so please bear
with me!

Introduction
===========

Bruge hits the grounds running with this chapter, making it clear right from the
beginning that the style of programming this book advocates is a bit different
from what the reader may be used to. On the very first page he states:

> All the linguistic devices introduced [in this book] are based upon two
> methods of constructing expressions from smaller expressions [...] Thus the
> extra notation that is added to this basis adds no new structural features

He names the extra notation 'additions'. These days we would call it 'syntactic
sugar', but the idea is identical: New forms of expression that can be
translated to the core constructs of the language.  That's pretty heavy, all of
what's about to be described can be reduced to two constructs that he describes
as follows:

1. "An operator/operand construction that denotes function application"
2. "An expression format which denotes a function"

Burge does remind the reader that there will be some constants (read primitives)
that are required for certain tasks. This sets up one of the first great
insights of the text, stating that the constructs

> create a practical and powerful programming system, which is more like a
> _family_ of programming languages than a single language, because the features
> introduced are concerned more with combining functions to produce new ones
> than with the nature of the primitive functions that are being combined.

Here comes the kicker:

> A programming language for a particular range of applications can be obtained
> by adding an appropriate set of primitives to this basic structure.

So in just under a page Burge has set the stage for understanding functional
programming through the lens of the Lambda Calculus and its usefulness in
creating DSLs but avoids making any distinction between the three. So Burge's lesson
so far: understand two core constructs, then pick primitives according to your
domain.

We haven't gotten much further in the decades since.

A common library design approach is just this: pick a few core constructs and then expose a
core set of 'primitives' that work over those constructs.[^2]

[^2]: There are other good approaches to library design too, but this is the
basic idea behind a good interface.


He's not done yet, there's one more insight waiting for us, right before he
starts introducing his language he reminds us that the focus is on _expressions_
and not _mechanisms_. He argues that expressions have a great property:

> the value, or meaning, of an expression depends in a simple way only on the
> values of its subexpressions and on no other properties of them.

So again, without leaning on fancy language, Burge lays it all out in front of
us. He states that this property allows you solve large and complex problems
by breaking them down into small, simple, and independent problems. And

> it is possible to make the structure of the program match the structure of the
> problem being solved.


The Language
============

Operator/Operand Expressions
----------------------------

Let's start with the obvious: Functional Programming deals with functions.
Because of this it's very important to specify what we mean when we say
'function'. Burge does this by talking about the relationship a function has to
its arguments. More specifically, he talks about types. But remember, this is a
book from decades ago, so we're not bringing in any heavy machinery here. He
starts with function types.

Before we go any further I would like to point out that Burge never talks about
type-checking or ever implementing a type-system. So why introduce types at all?
We'll types help guide our thinking, even if we don't statically enforce them.
Many of the most accomplished LISP and Racket programmers I know think about
types _all the time_. Just because your compiler doesn't do type checking
doesn't mean you can't benefit from thinking about types.

### Functions and types

I've been going on about _expressions_ over _mechanisms_ for a while and I
haven't even shown you one yet. So let's take a look at one: $f(x)$. This is how
Burge typesets the application of the function $f$ to the value $x$.  This is
the first expression we've seen, so let's unpack it. $f(x)$ is a function
application, which is a type of _compound_ expression. Function applications are
made up of two parts: an _operator_ (in this case $f$), and an _operand_ (in
this case $x$).

In the introduction I claimed that all of the language constructs we will
introduce can be understood with just two concepts, function application is the
first one! The basic idea of looking at application as a compound expression is
that in order to make sense of the expression $f(x)$ you're going to have to
make sense of $f$ and of $x$. That may seem obvious, but the insight Burge is
trying to get across to you is that if you program in this expression-based
style you make sense of them each in isolation. That's a very powerful idea!

Okay, so a function is a value that you can _apply_ to other values (a.k.a. the
inputs).  However, not all values make sense as inputs; for example, the
function $square$ that takes a number and multiplies it by itself, does not make
much sense if you give it the letter 'a' as an input. So, let's be a bit more
concrete. A function has a type, usually written as

$$A \longrightarrow B$$

Here the $(\rightarrow)$ is what indicates this is a function. The $A$ is
the types of values it accepts, and $B$ is the type of values it returns. Burge
calls these the _domain_ and _range_, respectively.[^3] So if you have a
function $f$ of type $A \rightarrow B$, and you apply it to a value $x$
which has type $A$, the result is of type $B$. Burge typesets function
application as both $f(x)$ or $f\ x$.

[^3]: Note that in modern programming language texts and papers _range_ would
almost universally be called the _codomain_. The reasoning is simple: The range
of a function is the set of values it _actually_ returns, whereas the codomain
is the set of _possible_ return values. So when reasoning about types we're
actually reasoning about the codomain.

Earlier I mentioned the function $square$, one possible type would be

$$square \in (\text{integer} \rightarrow \text{integer})$$

Other common examples:

$$ 
sin \in (\text{real} \rightarrow \text{real}) \\
log \in (\text{positive} \rightarrow \text{real}) \\
negate \in (\text{positive} \rightarrow \text{negative})
$$

So in general anything of the form $g \in (A \rightarrow B)$ is an assertion
that $g$ is a function that takes arguments of type $A$ and returns values of
type $B$. This means if we have a value $x$ of type $A$, we can be assured that
$g(x) \in B$.

Okay, this is all well and good, but every function we've looked at takes only a
single argument, what about things like $+$? Here are some examples:

$$
+ \in (\text{real} \times \text{real} \rightarrow \text{real})\\
min \in (\text{real} \times \text{real} \rightarrow \text{real})\\
equal \in (\text{real} \times \text{real} \rightarrow \text{truth value})
$$

This is just saying that these functions take two arguments, both reals, and
return a single value. This generalizes in the obvious way to functions with an
arbitrary number of arguments. So a function that takes $N$ arguments would have
a type like

$$A_{1} \times A_{2} \times \dots \times A_{N} \rightarrow B$$

Where A_{i} is the appropriate type of the $i^{th}$ argument.

Now, the obvious way to apply functions that take multiple arguments is by just
extending the syntax we already have, giving us $min(x,y)$ or $+(x,y)$. Many
languages have a predetermined set of special functions that can be applied
differently, the arithmetic operations are usually such an exception, so you
could write $x + y$ instead of $+(x,y)$. Burge is no different here and his
language allows for certain functions to be applied in this special manner.

It's worth pointing out that even if you have a special syntax for the
application of certain functions, _the operator/operand relationship is
unchanged_. It could be argued that allowing any such special syntax obscures
this relationship and therefore obscures the meaning of the expression. The
important bit is that regardless of the syntax it is crucial that you be able to
identify the operator and the operand of a function application.

### Meaning of expressions

We now know what makes up a function application, but we still don't know the
_meaning_ of anything. I claimed earlier that you can determine the meaning of a
function application by finding the meaning of its operator and operand. But
eventually you'll reach an expression that is not a function application, Burge
calls these _simple_ expressions. In this section we will explain the meaning of
on type of simple expression: _constants_.

In any programming system there is a set of constants. The meaning of these
constants is given by the system and its implementation. The most obvious
constants are things like numbers: $4$, $-128$, etc.[^5] or arithmetic
operations: $+$, $-$, $\times$, etc.

[^5]: The constants are defined by the system in use and aren't necessarily
equivalent to their pure mathematical counterparts. For example adding two
numbers in many languages introduces the possibility of overflow, whereas the
'true' $+$ has no such possibility. Or how some languages automatically convert
all numeric values to floating point, etc.

So if your language has the constant $+$ which takes two numbers and adds them
together, and it has numeric constants we can now find the meaning of
expressions like $+(4,5)$, or $square(\times(2,3))$.

We do this with the following procedure:

1. If the expression is simple, what is the meaning of the constant?
2. If the expression is composite, what is the meaning of its operator, and what
is the meaning of its operand?

Try applying this procedure to $+(4,5)$, or $square(\times(2,3))$. Do yourself a
favor and force yourself to actually go through the steps. In this instance it
is not very hard, but it is the practice of finding the meaning of expressions
via the meaning of their constituent parts that is important.

### Functions are values too

Variables and Lambdas
---------------------

Additional Forms
----------------

### Aside

It may not seem like much, but Burge is doing something very significant with
the way he is presenting all this information to us. In some ways it's very
strange, he hasn't really shown us very much about this language at all. What he
has shown us though is that for everything that is introduced, he provides us
with a way to reason about it. He spends 



Epilogue
--------

-JMCT
